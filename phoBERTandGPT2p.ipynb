{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Kiểm tra phiên bản PyTorch\n",
    "print(torch.cuda.is_available())  # Kiểm tra GPU có hoạt động không\n",
    "# print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU (nếu có)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences start: Debug là việc thường xuyên của delevoper.\n",
      "tokens list :  [0, 44204, 2302, 8, 49, 311, 2121, 7, 13815, 1358, 7409, 4912, 5, 2]\n",
      "decode ngược lại tokenize  <s> Debug là việc thường xuyên của delevoper. </s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phoBERT = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "custokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "# ------add token ('\\n') to enter lines --------#\n",
    "custokenizer.add_tokens('\\n')\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Debug là việc thường xuyên của delevoper.\"\n",
    "print('Sequences start:', line)\n",
    "#-------------encode --------------#\n",
    "tokens = custokenizer.encode(line)\n",
    "print('tokens list : ', tokens)\n",
    "#-----------Decode ngược lại thành câu từ chuỗi index token---------------#\n",
    "print('decode ngược lại tokenize ', custokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Input file path data_train_process.txt not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 129\u001b[0m\n\u001b[1;32m    127\u001b[0m train_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_train_process.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    128\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_test_process.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 129\u001b[0m train_dataset,test_dataset,data_collator \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcustokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#-----------Test dataloader----------------#\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_dataset))\n",
      "Cell \u001b[0;32mIn[6], line 113\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(train_path, test_path, custokenizer)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_dataset\u001b[39m(train_path, test_path, custokenizer):\n\u001b[0;32m--> 113\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPoemDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m          \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m          \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#256\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m PoemDataset(\n\u001b[1;32m    119\u001b[0m           tokenizer\u001b[38;5;241m=\u001b[39mcustokenizer,\n\u001b[1;32m    120\u001b[0m           file_path\u001b[38;5;241m=\u001b[39mtest_path,\n\u001b[1;32m    121\u001b[0m           block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m)   \n\u001b[1;32m    123\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[1;32m    124\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mcustokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mPoemDataset.__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     29\u001b[0m     tokenizer: PreTrainedTokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     cache_dir: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m ):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m block_size \u001b[38;5;241m-\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mnum_special_tokens_to_add(pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     directory, filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(file_path)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input file path data_train_process.txt not found"
     ]
    }
   ],
   "source": [
    "#--------------Create Dataset----------------#\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from filelock import FileLock\n",
    "from transformers.utils import logging\n",
    "from typing import Dict, List, Optional\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tokenizers : is pretrain tokenizer of PhoBERT\n",
    "    file_path  : path to file train, test\n",
    "    block_size : size of 1 block , optinal\n",
    "    cache_dir  : just load 1 once and saved\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else directory,\n",
    "            \"cached_lm_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # -----------Make sure only the first process in distributed training processes the dataset,----------------#\n",
    "        # ---------------------------------------and the others will use the cache------------------------#\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "                self.examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                #-----convert text to tokenizers----------------------------#\n",
    "                '''\n",
    "                1. Convert word -> subword (tokenizer.tokenize(text))\n",
    "                2. COnvert subword -> number (tokenizer.convert_tokens_to_ids)\n",
    "                '''\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "                # ------------- Truncate in block of block_size-----------------#\n",
    "                #-----------Beacuse add_token('\\n') -> inds = 64001------------#\n",
    "                #--------If len(block_size)>56 so cut and add_special_tokens (<s>, </s>)---------------#\n",
    "                i = 0\n",
    "                while i < len(tokenized_text) - block_size + 1:\n",
    "                    inds = tokenized_text[i : i + block_size]\n",
    "                    for j in range(0, len(inds)):\n",
    "                        if inds[j]==64001:\n",
    "                            inds = inds[j+1:] #remove the first \\n\n",
    "                            break\n",
    "                    for j in range(len(inds)-1, 0, -1):\n",
    "                        if inds[j]==64001:\n",
    "                            inds = inds[:j-1] #remove \\n\n",
    "                            break\n",
    "                    i += len(inds)\n",
    "                    self.examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(inds)\n",
    "                    )\n",
    "                    \n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    " #-----------Load dataset-----------------------#\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling, LineByLineWithSOPTextDataset\n",
    "\n",
    "def load_dataset(train_path, test_path, custokenizer):\n",
    "    train_dataset = PoemDataset(\n",
    "          tokenizer=custokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size= 56)#256\n",
    "     \n",
    "    test_dataset = PoemDataset(\n",
    "          tokenizer=custokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=56)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=custokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "train_path = 'data_train_process.txt'\n",
    "test_path = 'data_test_process.txt'\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,custokenizer)\n",
    "#-----------Test dataloader----------------#\n",
    "print(len(test_dataset))\n",
    "print(len(train_dataset))\n",
    "#-------------Test decode to sentence ---------------#\n",
    "print(custokenizer.decode(test_dataset[7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7029, 0.5997, 0.6612,  ..., 0.2412, 0.2251, 0.4816],\n",
      "        [0.0476, 0.2268, 0.5549,  ..., 0.5241, 0.7206, 0.6583],\n",
      "        [0.6434, 0.5206, 0.5127,  ..., 0.6628, 0.8305, 0.6344],\n",
      "        ...,\n",
      "        [0.5905, 0.1016, 0.0495,  ..., 0.5458, 0.4329, 0.6145],\n",
      "        [0.2249, 0.9541, 0.9995,  ..., 0.3027, 0.6147, 0.4669],\n",
      "        [0.3857, 0.3220, 0.6545,  ..., 0.4365, 0.1305, 0.5388]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Config, GPT2LMHeadModel\n",
    "#--------------------------Load  pretrain model GPT-2--------------------#\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Random weights => fine-turning model\n",
    "rand_weight = torch.rand(model_gpt2.lm_head.weight.shape)\n",
    "print(rand_weight)\n",
    "model_gpt2.lm_head.weight = torch.nn.parameter.Parameter(rand_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Because GPT2 has vocabulary_size 50257 and (wte): Embedding(50257, 768)\n",
    "So  convert vocabulary_size= 64002, Embedding(64002, 768)\n",
    "'''\n",
    "task_gpt2 = {\"text-generation\": {\"do_sample\": True, \"max_length\": 56}} #edit output size\n",
    "config_gpt2 = configuration = GPT2Config(vocab_size=64002, n_positions=58, n_ctx=58,\n",
    "                           task_specific_params=task_gpt2,\n",
    "                           eos_token_id = 2,\n",
    "                           bos_token_id = 0,\n",
    "                           pad_token_id = 1,\n",
    "                           sep_token_id = 2,\n",
    "                          #  eos_token_id=custokenizer.eos_token_id,\n",
    "                          #  bos_token_id=custokenizer.bos_token_id, \n",
    "                          #  pad_token_id=custokenizer.pad_token_id,\n",
    "                          #  sep_token_id=custokenizer.sep_token_id\n",
    "                           )\n",
    "model_gpt2 = GPT2LMHeadModel(config_gpt2)\n",
    "model_gpt2\n",
    "#save model_gpt2 (vocabulary_size =64002)\n",
    "model_gpt2.save_pretrained('save_modelGPT2/')\n",
    "task = {\"text-generation\": {\"do_sample\": True, \"max_length\": 56}} #edit output size\n",
    "configuration = GPT2Config(vocab_size=64002, n_positions=58, n_ctx=58,\n",
    "                           task_specific_params=task,\n",
    "                           eos_token_id = 2,\n",
    "                           bos_token_id = 0,\n",
    "                           pad_token_id = 1,\n",
    "                           sep_token_id = 2,\n",
    "                          #  eos_token_id=custokenizer.eos_token_id,\n",
    "                          #  bos_token_id=custokenizer.bos_token_id, \n",
    "                          #  pad_token_id=custokenizer.pad_token_id,\n",
    "                          #  sep_token_id=custokenizer.sep_token_id\n",
    "                           )\n",
    "poem = GPT2LMHeadModel(configuration)\n",
    "\n",
    "# Load weights of model_gpt2 ( random weights)\n",
    "load_model_gpt2 = GPT2LMHeadModel.from_pretrained('save_modelGPT2/')\n",
    "poem.load_state_dict(load_model_gpt2.state_dict())\n",
    "#-----------Print process training ------------#\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import pipeline\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        if int(state.epoch)%10==0:\n",
    "            pipe = pipeline('text-generation', model=model, tokenizer=custokenizer, device=0)\n",
    "            with open(\"sample.txt\", \"a\") as f:\n",
    "                f.write(pipe('<s> tìm về một thuở hạ xưa')[0]['generated_text'])\n",
    "                f.write(\"\\n===========================================\\n\")\n",
    "                f.close()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-poem\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=100, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training  \n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    save_steps=5000, # after # steps model is saved \n",
    "    save_total_limit = 2, # delete other checkpoints\n",
    "    warmup_steps=5000,    # number of warmup steps for learning rate scheduler\n",
    "    # logging_dir='/content/drive/MyDrive/BERT/gpt2-poem/logs', # directory for storing logs\n",
    "    logging_steps=5000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (2.6.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "trainer = Trainer(\n",
    "    model=poem, # GPT2\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks = [PrinterCallback],\n",
    ")\n",
    "# # -------Train and save model-----------#\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>cuộc sống nên người \n",
      " đường đi dễ khéo cả đời cho xong \n",
      " trăm năm một giấc mơ hồng \n",
      " lòng đau trót ngộ lấp sông tiền đường \n",
      " này đây cách biệt quan\n"
     ]
    }
   ],
   "source": [
    "#-------Load model saved-----------------#\n",
    "from transformers import pipeline\n",
    "poem = pipeline('text-generation', model=\"../test/gpt2-poem\", tokenizer=custokenizer)\n",
    "#Test\n",
    "a = poem('<s>cuộc sống')\n",
    "print(a[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
