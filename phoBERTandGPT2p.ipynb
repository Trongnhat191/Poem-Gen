{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n",
      "NVIDIA GeForce GTX 1650 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Kiểm tra phiên bản PyTorch\n",
    "print(torch.cuda.is_available())  # Kiểm tra GPU có hoạt động không\n",
    "print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU (nếu có)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences start: Debug là việc thường xuyên của delevoper.\n",
      "tokens list :  [0, 44204, 2302, 8, 49, 311, 2121, 7, 13815, 1358, 7409, 4912, 5, 2]\n",
      "decode ngược lại tokenize  <s> Debug là việc thường xuyên của delevoper. </s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phoBERT = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "custokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "# ------add token ('\\n') to enter lines --------#\n",
    "custokenizer.add_tokens('\\n')\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Debug là việc thường xuyên của delevoper.\"\n",
    "print('Sequences start:', line)\n",
    "#-------------encode --------------#\n",
    "tokens = custokenizer.encode(line)\n",
    "print('tokens list : ', tokens)\n",
    "#-----------Decode ngược lại thành câu từ chuỗi index token---------------#\n",
    "print('decode ngược lại tokenize ', custokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n",
      "11072\n",
      "<s> đó là bến đậu của ta \n",
      " đó là tiếng hát đó là tứ thơ \n",
      " đó là chỗ hết vẩn vơ \n",
      " để ta mãi mãi đáng chờ đợi nhau \n",
      " nơi đây bao chàng ra </s>\n"
     ]
    }
   ],
   "source": [
    "#--------------Create Dataset----------------#\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from filelock import FileLock\n",
    "from transformers.utils import logging\n",
    "from typing import Dict, List, Optional\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tokenizers : is pretrain tokenizer of PhoBERT\n",
    "    file_path  : path to file train, test\n",
    "    block_size : size of 1 block , optinal\n",
    "    cache_dir  : just load 1 once and saved\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else directory,\n",
    "            \"cached_lm_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # -----------Make sure only the first process in distributed training processes the dataset,----------------#\n",
    "        # ---------------------------------------and the others will use the cache------------------------#\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "                self.examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                #-----convert text to tokenizers----------------------------#\n",
    "                '''\n",
    "                1. Convert word -> subword (tokenizer.tokenize(text))\n",
    "                2. COnvert subword -> number (tokenizer.convert_tokens_to_ids)\n",
    "                '''\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "                # ------------- Truncate in block of block_size-----------------#\n",
    "                #-----------Beacuse add_token('\\n') -> inds = 64001------------#\n",
    "                #--------If len(block_size)>56 so cut and add_special_tokens (<s>, </s>)---------------#\n",
    "                i = 0\n",
    "                while i < len(tokenized_text) - block_size + 1:\n",
    "                    inds = tokenized_text[i : i + block_size]\n",
    "                    for j in range(0, len(inds)):\n",
    "                        if inds[j]==64001:\n",
    "                            inds = inds[j+1:] #remove the first \\n\n",
    "                            break\n",
    "                    for j in range(len(inds)-1, 0, -1):\n",
    "                        if inds[j]==64001:\n",
    "                            inds = inds[:j-1] #remove \\n\n",
    "                            break\n",
    "                    i += len(inds)\n",
    "                    self.examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(inds)\n",
    "                    )\n",
    "                    \n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    " #-----------Load dataset-----------------------#\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling, LineByLineWithSOPTextDataset\n",
    "\n",
    "def load_dataset(train_path, test_path, custokenizer):\n",
    "    train_dataset = PoemDataset(\n",
    "          tokenizer=custokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size= 56)#256\n",
    "     \n",
    "    test_dataset = PoemDataset(\n",
    "          tokenizer=custokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=56)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=custokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "train_path = 'data_train_process.txt'\n",
    "test_path = 'data_test_process.txt'\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,custokenizer)\n",
    "#-----------Test dataloader----------------#\n",
    "print(len(test_dataset))\n",
    "print(len(train_dataset))\n",
    "#-------------Test decode to sentence ---------------#\n",
    "print(custokenizer.decode(test_dataset[7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2618, 0.9506, 0.2054,  ..., 0.3031, 0.0281, 0.5797],\n",
      "        [0.7797, 0.0898, 0.0015,  ..., 0.7293, 0.9260, 0.7904],\n",
      "        [0.1043, 0.1165, 0.9345,  ..., 0.2244, 0.5189, 0.2699],\n",
      "        ...,\n",
      "        [0.7151, 0.5862, 0.7578,  ..., 0.1758, 0.1858, 0.0608],\n",
      "        [0.9094, 0.5732, 0.1484,  ..., 0.1267, 0.2744, 0.8599],\n",
      "        [0.8391, 0.3510, 0.5439,  ..., 0.2870, 0.9996, 0.9308]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Config, GPT2LMHeadModel\n",
    "#--------------------------Load  pretrain model GPT-2--------------------#\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Random weights => fine-turning model\n",
    "rand_weight = torch.rand(model_gpt2.lm_head.weight.shape)\n",
    "print(rand_weight)\n",
    "model_gpt2.lm_head.weight = torch.nn.parameter.Parameter(rand_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Because GPT2 has vocabulary_size 50257 and (wte): Embedding(50257, 768)\n",
    "So  convert vocabulary_size= 64002, Embedding(64002, 768)\n",
    "'''\n",
    "task_gpt2 = {\"text-generation\": {\"do_sample\": True, \"max_length\": 56}} #edit output size\n",
    "config_gpt2 = configuration = GPT2Config(vocab_size=64002, n_positions=58, n_ctx=58,\n",
    "                           task_specific_params=task_gpt2,\n",
    "                           eos_token_id = 2,\n",
    "                           bos_token_id = 0,\n",
    "                           pad_token_id = 1,\n",
    "                           sep_token_id = 2,\n",
    "                          #  eos_token_id=custokenizer.eos_token_id,\n",
    "                          #  bos_token_id=custokenizer.bos_token_id, \n",
    "                          #  pad_token_id=custokenizer.pad_token_id,\n",
    "                          #  sep_token_id=custokenizer.sep_token_id\n",
    "                           )\n",
    "model_gpt2 = GPT2LMHeadModel(config_gpt2)\n",
    "model_gpt2\n",
    "#save model_gpt2 (vocabulary_size =64002)\n",
    "model_gpt2.save_pretrained('save_modelGPT2/')\n",
    "task = {\"text-generation\": {\"do_sample\": True, \"max_length\": 56}} #edit output size\n",
    "configuration = GPT2Config(vocab_size=64002, n_positions=58, n_ctx=58,\n",
    "                           task_specific_params=task,\n",
    "                           eos_token_id = 2,\n",
    "                           bos_token_id = 0,\n",
    "                           pad_token_id = 1,\n",
    "                           sep_token_id = 2,\n",
    "                          #  eos_token_id=custokenizer.eos_token_id,\n",
    "                          #  bos_token_id=custokenizer.bos_token_id, \n",
    "                          #  pad_token_id=custokenizer.pad_token_id,\n",
    "                          #  sep_token_id=custokenizer.sep_token_id\n",
    "                           )\n",
    "poem = GPT2LMHeadModel(configuration)\n",
    "\n",
    "# Load weights of model_gpt2 ( random weights)\n",
    "load_model_gpt2 = GPT2LMHeadModel.from_pretrained('save_modelGPT2/')\n",
    "poem.load_state_dict(load_model_gpt2.state_dict())\n",
    "#-----------Print process training ------------#\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import pipeline\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        if int(state.epoch)%10==0:\n",
    "            pipe = pipeline('text-generation', model=model, tokenizer=custokenizer, device=0)\n",
    "            with open(\"sample.txt\", \"a\") as f:\n",
    "                f.write(pipe('<s> tìm về một thuở hạ xưa')[0]['generated_text'])\n",
    "                f.write(\"\\n===========================================\\n\")\n",
    "                f.close()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-poem\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=100, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training  \n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    save_steps=5000, # after # steps model is saved \n",
    "    save_total_limit = 2, # delete other checkpoints\n",
    "    warmup_steps=5000,    # number of warmup steps for learning rate scheduler\n",
    "    # logging_dir='/content/drive/MyDrive/BERT/gpt2-poem/logs', # directory for storing logs\n",
    "    logging_steps=5000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (2.6.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trongnhat/anaconda3/envs/poem/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='193' max='138400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   193/138400 01:05 < 13:07:59, 2.92 it/s, Epoch 0.14/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mpoem, \u001b[38;5;66;03m# GPT2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [PrinterCallback],\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# -------Train and save model-----------#\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[0;32m~/anaconda3/envs/poem/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/poem/lib/python3.9/site-packages/transformers/trainer.py:2550\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m-> 2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "trainer = Trainer(\n",
    "    model=poem, # GPT2\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks = [PrinterCallback],\n",
    ")\n",
    "# -------Train and save model-----------#\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Load model saved-----------------#\n",
    "from transformers import pipeline\n",
    "poem = pipeline('text-generation', model=\"/content/drive/MyDrive/BERT/gpt2-poem\", tokenizer=custokenizer, config={'max_length':56})\n",
    "#Test\n",
    "a = poem('<s>cuộc sống')\n",
    "print(a[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
